{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# этот блок нужен, если потребуется что то подгрузить через pip\n",
        "# расчет на потенциальные модификации)\n",
        "\n",
        "# общая идея решения записана в README в директории проекта"
      ],
      "metadata": {
        "id": "DgWehs4yDHXH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import csv\n",
        "\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "HfkrHpGzu763"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dictionary(file_path: str, encod: str='utf-8') -> set:\n",
        "\n",
        "    \"\"\"Загрузка словаря с возможностью обработки разных кодировок\"\"\"\n",
        "\n",
        "    # Изначально пытаемся открыть словарь в традиционном utf-8\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encod) as f:\n",
        "            # Приводим все слова к нижнему регистру для сокращения количества\n",
        "            # разных символов и загружаем в множество для быстрого поиска\n",
        "            return set(word.strip().lower() for word in f)\n",
        "\n",
        "    # В случае, если utf-8 не обработает текс (такое может быть), выбираем\n",
        "    # другую кодировку для русского языка\n",
        "    except UnicodeDecodeError:\n",
        "        encodings = ['cp1251', 'iso-8859-5', 'koi8-r']\n",
        "        for enc in encodings:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=enc) as f:\n",
        "                    # Вернуть множество слов\n",
        "                    return set(word.strip().lower() for word in f)\n",
        "\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "\n",
        "        # В крайнем случае выводим исключение, что текст нечитаем\n",
        "        # (своеобразная защита от мусора)\n",
        "        raise ValueError(\"Не удалось декодировать файл словаря\")"
      ],
      "metadata": {
        "id": "UDVvXRd4u-Fc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_segmentation(text: str, dictionary: set, max_word_len: int=20) -> list:\n",
        "\n",
        "    \"\"\"Алгоритм сегментации текста\"\"\"\n",
        "\n",
        "    n = len(text)\n",
        "\n",
        "    # Ищем наилучшее разбиение, прменяя динамическое программирование\n",
        "    # В dp будем хранить оценки оптимальности разбиения до позиции i (dp[i])\n",
        "    dp = [-10**9] * (n + 1)\n",
        "    dp[0] = 0\n",
        "    prev = [-1] * (n + 1)\n",
        "\n",
        "    # Заполняем таблицу оценок\n",
        "    # Внешний цикл:\n",
        "    # Перебирает все возможные начальные позиции разбиения\n",
        "    # Пропускает недостижимые позиции (dp[i] == -10**9)\n",
        "    # Внутренний цикл:\n",
        "    # Рассматривает подстроки от i до j, где j меняется\n",
        "    # от i+1 до min(n, i + max_word_len)\n",
        "    # max_word_len ограничивает максимальную длину рассматриваемого слова\n",
        "\n",
        "    # Решение основано на Принципе оптимальности Беллмана, которое\n",
        "    # в контексте задачи формулируется как:\n",
        "    # Оптимальное разбиение текста[0:n] содержит\n",
        "    # оптимальное разбиение текста[0:k] для некоторого k < n.\n",
        "\n",
        "    for i in range(n + 1):\n",
        "        if dp[i] == -10**9:\n",
        "            continue\n",
        "\n",
        "        for j in range(i + 1, min(n, i + max_word_len) + 1):\n",
        "            word = text[i:j].lower()\n",
        "            # Начисляем \"бонусы\", если слово есть в словаре\n",
        "            bonus = len(word)**2 if word in dictionary else 0\n",
        "            # \"Штраф\", если слова неизвестны\n",
        "            penalty = -0.5 * len(word) if word not in dictionary else 0\n",
        "\n",
        "            # обновляем информацию о разбиении при нахождении решения\n",
        "            if dp[j] < dp[i] + bonus + penalty:\n",
        "                dp[j] = dp[i] + bonus + penalty\n",
        "                prev[j] = i\n",
        "\n",
        "    positions = []\n",
        "    i = n\n",
        "\n",
        "    # По сотавленной таблице пытаемся восстановить пробелы в тексте\n",
        "    # Начинаем с конца текста (i = n)\n",
        "    # Двигаемся назад по массиву prev\n",
        "    # Для каждого перехода prev[i] → i добавляем позицию i в список\n",
        "    # Исключаем позицию n (конец текста)\n",
        "    # Разворачиваем список, так как мы шли с конца\n",
        "\n",
        "    while i > 0:\n",
        "        if prev[i] == -1:\n",
        "            # Если не нашли разбиение, пропускаем один символ\n",
        "            i -= 1\n",
        "            continue\n",
        "\n",
        "        if i != n:\n",
        "            positions.append(i)\n",
        "\n",
        "        i = prev[i]\n",
        "\n",
        "    # Разворачиваем массив\n",
        "    positions.reverse()\n",
        "\n",
        "    # Вернуть список позиций пробела\n",
        "    return positions"
      ],
      "metadata": {
        "id": "TBKO1TQMvA9Y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(input_file: str, output_file: str, dictionary_path: str):\n",
        "\n",
        "    \"\"\"Непостредственно обработка файла с текстом\"\"\"\n",
        "\n",
        "    # Загружаем словарь\n",
        "    dictionary = load_dictionary(dictionary_path)\n",
        "\n",
        "    # Кодировки, которые могут использоваться при открытии файла\n",
        "    encodings = ['utf-8', 'cp1251', 'iso-8859-5', 'koi8-r']\n",
        "\n",
        "    # Пытаемся открыть разными кодировками\n",
        "    for encod in encodings:\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding=encod) as f:\n",
        "                lines = f.readlines()\n",
        "            break\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    # в крайнем случае выводим исключение, что текст нечитаем\n",
        "    # (снова защита от мусора)\n",
        "    else:\n",
        "        raise ValueError(\"Не удалось декодировать входной файл\")\n",
        "\n",
        "    # Обработка данных и запись результата в csv формат\n",
        "    # Выходной файл записываем в utf-8\n",
        "    with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'predicted_positions'])\n",
        "\n",
        "        # Здесь проходимся по строчкам файла\n",
        "        for line in lines[1:]:\n",
        "            line = line.strip()\n",
        "            # Разделяем ID и текст по первой запятой\n",
        "            parts = line.split(',', 1)\n",
        "\n",
        "            # Простейшая обработка пустых значений\n",
        "            # На валидационном датасете не пригодится, но потенциально может\n",
        "            # быть полезна\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "\n",
        "            id, text = parts\n",
        "            # Прогоняем каждую строку через сегментатор\n",
        "            positions = text_segmentation(text, dictionary)\n",
        "            # Результат - пишем в файл - profit\n",
        "            writer.writerow([id, json.dumps(positions)])"
      ],
      "metadata": {
        "id": "vsfMqThfvEY0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Точка запуска\n",
        "# Используем стандартную конструкцию, если хотим, чтобы наш код запускался как\n",
        "# модуль где-нибудь еще (можно и без этого, но на всякий)\n",
        "if __name__ == \"__main__\":\n",
        "    valid_dataset = 'dataset_1937770_3.txt'\n",
        "    output_dataset = 'output.csv'\n",
        "    vacab = 'russian.txt'\n",
        "\n",
        "    process_file(valid_dataset, output_dataset, vacab)"
      ],
      "metadata": {
        "id": "5aKvG10wvHCO"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}